{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:21.585881Z","iopub.status.busy":"2023-07-05T09:15:21.585516Z","iopub.status.idle":"2023-07-05T09:15:21.595022Z","shell.execute_reply":"2023-07-05T09:15:21.594098Z","shell.execute_reply.started":"2023-07-05T09:15:21.585849Z"},"trusted":true},"outputs":[],"source":["import os\n","import time\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from datetime import datetime\n","from gensim.models import Word2Vec\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, Input, GaussianNoise\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from sklearn.utils import shuffle\n","from collections import deque\n","from river import utils\n","from river import stats"]},{"cell_type":"markdown","metadata":{},"source":["# Read Data "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#innit variables for algortihm\n","GP_length = 2 # length of the traces\n","GP = 100 #how many traces with length GP_length should be present before the algortihm starts\n","w_cases = 100 #maximum number of traces present in SW\n","w_events = 1000 #maximum number of events present in SW\n","alpha = 0.1\n","\n","# Parameters for Word2Vec encoding\n","vector_size = 100\n","window = 5\n","min_count = 1\n","\n","dir_datasets = '_Data/Later/Real'"]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:21.855585Z","iopub.status.busy":"2023-07-05T09:15:21.855259Z","iopub.status.idle":"2023-07-05T09:15:27.024069Z","shell.execute_reply":"2023-07-05T09:15:27.023123Z","shell.execute_reply.started":"2023-07-05T09:15:21.855554Z"},"trusted":true},"outputs":[],"source":["def create_model(cases, size, window, min_count):\n","    model = Word2Vec(\n","                vector_size=size,\n","                window=window,\n","                min_count=min_count)\n","    sentences = []\n","    for group in cases:\n","        group_sentences = []\n","        for row in group:\n","            row_sentences = [str(item) for item in row]\n","            group_sentences.extend(row_sentences)\n","        sentences.append(group_sentences)\n","    \n","    model.build_vocab(sentences)\n","    model.train(sentences, total_examples=len(sentences), epochs=10)\n","    return model\n","\n","def train_model(model, sequence): \n","    sequence = [[attribute for event in sequence for attribute in event]]\n","    model.train(sequence, total_examples=1, epochs=1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:27.025785Z","iopub.status.busy":"2023-07-05T09:15:27.025456Z","iopub.status.idle":"2023-07-05T09:15:27.031874Z","shell.execute_reply":"2023-07-05T09:15:27.030981Z","shell.execute_reply.started":"2023-07-05T09:15:27.025754Z"},"trusted":true},"outputs":[],"source":["def concat_feature_vector(events, model):\n","    vectors = []\n","    for event in events:\n","        case_vector = []\n","        for token in event:\n","            try:\n","                case_vector.append(model.wv[token])\n","            except KeyError:\n","                pass\n","        embedded_event = np.array(case_vector).mean(axis=0)\n","        vectors.append(embedded_event)\n","\n","    embedded_sequence = np.array(vectors)\n","    embedded_sequence = np.reshape(embedded_sequence, (vector_size*len(vectors)))\n","    return embedded_sequence"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def make_model(nr_features, hidden_layers, hidden_size_factor=0.5, noise=None):\n","    '''\n","    Create the DAE model\n","    '''\n","    input_ = Input(shape=(nr_features,), name='input')\n","    x = input_\n","\n","    if noise is not None:\n","        x = GaussianNoise(noise)(x) \n","\n","    for i in range(hidden_layers):\n","        if isinstance(hidden_size_factor, list):\n","            factor = hidden_size_factor[i]\n","        else:\n","            factor = hidden_size_factor\n","        x = Dense(int(nr_features * factor), activation='relu', name=f'hid{i + 1}')(x)\n","        x = Dropout(0.5)(x)\n","\n","    output = Dense(nr_features, activation='tanh', name='output')(x)\n","\n","    model = Model(inputs=input_, outputs=output)\n","\n","    model.compile(\n","        optimizer=Adam(learning_rate=0.0001, beta_2=0.99),\n","        loss='mean_squared_error',\n","    )\n","\n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:27.068387Z","iopub.status.busy":"2023-07-05T09:15:27.067757Z","iopub.status.idle":"2023-07-05T09:15:27.082010Z","shell.execute_reply":"2023-07-05T09:15:27.081113Z","shell.execute_reply.started":"2023-07-05T09:15:27.068357Z"},"trusted":true},"outputs":[],"source":["# Function to compute the reconstruction error for a sequence using the autoencoder model\n","def compute_reconstruction_error(autoencoder, sequence):\n","    reconstructed_sequence = autoencoder(sequence, training=False)\n","    reconstruction_error = np.mean(np.square(sequence - reconstructed_sequence))\n","    return reconstruction_error"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def remove_oldest_event(current_id):\n","    global case_ids, cid_to_events, event_counter\n","\n","    cid_with_oldest_event = case_ids.popleft()\n","    while cid_with_oldest_event not in cid_to_events:\n","        cid_with_oldest_event = case_ids.popleft()\n","    cid_to_events[cid_with_oldest_event] = np.delete(cid_to_events[cid_with_oldest_event], 0, axis=0)\n","    if len(cid_to_events[cid_with_oldest_event]) == 0 and cid_with_oldest_event != current_id:\n","        del cid_to_events[cid_with_oldest_event]\n","        del cid_to_last_timestamp[cid_with_oldest_event]\n","    event_counter -= 1\n","\n","def save_new_event(cid, event_data, event_timestamp, after_GP: bool):\n","    global cid_to_events, cid_to_last_timestamp, case_ids, event_counter\n","    \n","    event_counter += 1\n","    case_ids.append(cid)\n","    \n","    if cid in cid_to_events.keys():\n","        if event_counter >= w_events and after_GP:\n","            remove_oldest_event(cid)\n","        sequence = cid_to_events[cid]\n","        sequence = np.vstack([sequence, event_data])\n","        cid_to_events[cid] = sequence\n","    else:\n","        number_of_cases = len(cid_to_events.keys())\n","        if number_of_cases >= w_cases and after_GP:\n","            min_timestamp = datetime.strptime('2100-01-01 00:00:00', '%Y-%m-%d %H:%M:%S') # for datasets with this timestamp format\n","            #min_timestamp = datetime.strptime('2100-01-01T00:00:00.111+09:00', '%Y-%m-%dT%H:%M:%S.%f%z')\n","            removing_cid = None\n","            for cid, latest_timestamp in cid_to_last_timestamp.items():\n","                timestamp = datetime.strptime(latest_timestamp, '%Y-%m-%d %H:%M:%S')\n","                #timestamp = datetime.strptime(latest_timestamp, '%Y-%m-%dT%H:%M:%S.%f%z')\n","                if timestamp < min_timestamp:\n","                    min_timestamp = timestamp\n","                    removing_cid = cid\n","            del cid_to_events[removing_cid]\n","            del cid_to_last_timestamp[removing_cid]\n","        if event_counter >= w_events and after_GP:\n","            remove_oldest_event(cid)\n","        cid_to_events[cid] = np.array([event_data])\n","        sequence = cid_to_events[cid]\n","        cid_to_last_timestamp[cid] = event_timestamp\n","    return sequence"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def pre_process_model_inputs(inputs, input_length_model):\n","    padded_vector= np.pad(inputs, (0, input_length_model - inputs.shape[0]), 'constant', constant_values=0)\n","    encoded_sequence = np.expand_dims(padded_vector, axis=0)\n","    return encoded_sequence\n","\n","def init_train_model(cid_to_events, input_dim, encoder):\n","    model = make_model(input_dim, hidden_layers=2)\n","    \n","    max_prefix_length = 0\n","    for v in cid_to_events.values():\n","        if len(v) > max_prefix_length:\n","            max_prefix_length = len(v) \n","    \n","    \n","    for nr_events in range(1, max_prefix_length+1):\n","        prefix_dict = {}\n","        for key, value in cid_to_events.items():\n","            if len(value) >= nr_events:\n","                prefix_dict[key] = value[:nr_events]\n","\n","        for sequence in prefix_dict.values():\n","            train_model(encoder, sequence)\n","            embedding = concat_feature_vector(sequence, encoder)\n","            tensor = pre_process_model_inputs(embedding, input_length_model = model.input.shape[1])\n","            model.train_on_batch(tensor, tensor)\n","            \n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:27.087334Z","iopub.status.busy":"2023-07-05T09:15:27.086547Z"},"trusted":true},"outputs":[{"ename":"PermissionError","evalue":"[Errno 13] Permission denied: 'results_w2v_bpic17-0.3-1.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m scoring_duration \u001b[39m=\u001b[39m start_time_label \u001b[39m-\u001b[39m start_time_scoring\n\u001b[0;32m     87\u001b[0m prediction_duration \u001b[39m=\u001b[39m end_time_label \u001b[39m-\u001b[39m start_time_label\n\u001b[1;32m---> 89\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(results_filename, \u001b[39m\"\u001b[39;49m\u001b[39ma+\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m csvfile:\n\u001b[0;32m     90\u001b[0m     csvfile\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcase_id\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mprefix_length\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00manomaly_score\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mthreshold\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mpredicted_label\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mencoding_duration\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mscoring_duration\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mprediction_duration\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39m#{mean_window},{std_window},\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'results_w2v_bpic17-0.3-1.csv'"]}],"source":["# Initialize DataFrame to store reconstruction errors\n","# reconstruction_error_df = pd.DataFrame(columns=['case_id', 'reconstruction_error'])\n","datasets = [dir_datasets + '/' + dataset for dataset in os.listdir(dir_datasets)]\n","for dataset in datasets:\n","    \n","    file_name = os.path.basename(dataset)\n","    df_dataset = pd.read_csv(dataset)\n","    df_dataset.sort_values('timestamp', inplace=True)\n","    #df_dataset = df_dataset[~df_dataset['case_id'].isin(df_dataset.groupby('case_id')['timestamp'].apply(lambda x: x.isna().any()).loc[lambda x: x].index)]\n","    df_dataset['timestamp'] = pd.to_datetime(df_dataset['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n","    df_dataset = df_dataset.drop(['isEndTrace'], axis=1)\n","    max_length = df_dataset.groupby('case_id').size().max()\n","    cases = df_dataset.groupby(df_dataset.columns[0]).apply(lambda x: x.iloc[:, ~df_dataset.columns.isin(['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly'])].values.tolist())\n","    word2vec = create_model(cases, vector_size, window, min_count)\n","    input_dim = vector_size\n","    results_filename = 'results_w2v_' + file_name\n","\n","    cid_to_events : dict[int, list]= dict()\n","    cid_to_last_timestamp : dict[int, float] = dict() # to know what is for a case id the timestamp of the last event (the most recent)\n","    case_ids = deque() # deque of the case ids such that, at each point, the popleft represents the oldest event\n","    event_counter = 0\n","    anomalous_cases = []\n","\n","    # Initialize DataFrame to store reconstruction errors\n","    sliding_windows_dae = {}\n","\n","    for i in range(max_length + 1):\n","        sliding_windows_dae[i] = utils.Rolling(stats.Var(), window_size=6000)\n","\n","    after_GP = False\n","    threshold = 1 # initial default value\n","\n","    for _, row in df_dataset.iterrows():\n","        \n","        #skip_training = False\n","        case_label = row['isAnomaly']\n","        case_id = row['case_id']\n","        timestamp = row['timestamp']\n","        prefix_length = row['event_position']\n","        anomaly_type = row['anomaly']\n","        event_data = row.drop(['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly']).values\n","        #event_data = row.iloc[list(range(2, 3)) + list(range(3, len(x.columns)))].values\n","\n","        sequence = save_new_event(case_id, event_data, timestamp, after_GP)\n","        \n","        if not after_GP:\n","\n","            if len([k for k, v in cid_to_events.items() if len(v) >= GP_length]) >= GP:\n","                after_GP = True\n","                autoencoder = init_train_model(cid_to_events, input_dim, encoder=word2vec)\n","        \n","        if after_GP:\n","            start_time_encoding = time.time()    \n","            # Encode the sequence using Word2Vec\n","            encoded_sequence = concat_feature_vector(sequence, word2vec)\n","            encoded_sequence = pre_process_model_inputs(encoded_sequence, input_length_model=autoencoder.input.shape[1])\n","            train_model(word2vec, sequence)\n","            start_time_scoring = time.time()\n","            \n","            anomaly_score = compute_reconstruction_error(autoencoder, encoded_sequence)\n","            as_window = sliding_windows_dae[prefix_length]\n","            std = np.sqrt(as_window.update(anomaly_score).get())\n","            mean = as_window.mean.get()\n","            threshold = mean + alpha * std\n","            \n","            if case_id in anomalous_cases:\n","                predicted_label = 1\n","                    \n","            else:\n","                predicted_label = 1 if anomaly_score >= threshold else 0\n","            \n","            #Saving the anomalous cases\n","            if predicted_label == 1: \n","                anomalous_cases.append(case_id)\n","            \n","            start_time_label = time.time()\n","            \n","            autoencoder.train_on_batch(encoded_sequence, encoded_sequence)\n","            end_time_label = time.time()\n","            encoding_duration = start_time_scoring - start_time_encoding\n","            scoring_duration = start_time_label - start_time_scoring\n","            prediction_duration = end_time_label - start_time_label\n","\n","            with open(results_filename, \"a+\") as csvfile:\n","                csvfile.write(f\"{case_id},{prefix_length},{anomaly_score},{threshold},{predicted_label}, {encoding_duration},{scoring_duration},{prediction_duration}\\n\") #{mean_window},{std_window},"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
