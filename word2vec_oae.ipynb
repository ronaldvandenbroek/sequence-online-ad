{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-05T09:15:21.585881Z",
     "iopub.status.busy": "2023-07-05T09:15:21.585516Z",
     "iopub.status.idle": "2023-07-05T09:15:21.595022Z",
     "shell.execute_reply": "2023-07-05T09:15:21.594098Z",
     "shell.execute_reply.started": "2023-07-05T09:15:21.585849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, GaussianNoise\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from river import utils, stats, metrics\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#innit variables for algortihm\n",
    "GP_length = 2 # length of the traces\n",
    "GP = 100 #how many traces with length GP_length should be present before the algortihm starts\n",
    "w_cases = 100 #maximum number of traces present in SW\n",
    "w_events = 1000 #maximum number of events present in SW\n",
    "alpha = 0.1\n",
    "\n",
    "# Parameters for Word2Vec encoding\n",
    "vector_size = 100\n",
    "window = 5\n",
    "min_count = 1\n",
    "\n",
    "dir_datasets = 'data/synthetic'\n",
    "# dir_datasets = 'data/real'\n",
    "\n",
    "dir_results = 'results/synthetic'\n",
    "# dir_results = 'results/real'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-05T09:15:21.855585Z",
     "iopub.status.busy": "2023-07-05T09:15:21.855259Z",
     "iopub.status.idle": "2023-07-05T09:15:27.024069Z",
     "shell.execute_reply": "2023-07-05T09:15:27.023123Z",
     "shell.execute_reply.started": "2023-07-05T09:15:21.855554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_model(cases, size, window, min_count):\n",
    "    model = Word2Vec(\n",
    "                vector_size=size,\n",
    "                window=window,\n",
    "                min_count=min_count)\n",
    "    sentences = []\n",
    "    for group in cases:\n",
    "        group_sentences = []\n",
    "        for row in group:\n",
    "            row_sentences = [str(item) for item in row]\n",
    "            group_sentences.extend(row_sentences)\n",
    "        sentences.append(group_sentences)\n",
    "    \n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "    return model\n",
    "\n",
    "def train_model(model, sequence): \n",
    "    sequence = [[attribute for event in sequence for attribute in event]]\n",
    "    model.train(sequence, total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-05T09:15:27.025785Z",
     "iopub.status.busy": "2023-07-05T09:15:27.025456Z",
     "iopub.status.idle": "2023-07-05T09:15:27.031874Z",
     "shell.execute_reply": "2023-07-05T09:15:27.030981Z",
     "shell.execute_reply.started": "2023-07-05T09:15:27.025754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def concat_feature_vector(events, model):\n",
    "    vectors = []\n",
    "    # print(\"Events:\", len(events), events)\n",
    "    for event in events:\n",
    "        case_vector = []\n",
    "        for token in event:\n",
    "            try:\n",
    "                case_vector.append(model.wv[token])\n",
    "            except KeyError as e:\n",
    "                print(\"Token not found:\", e)\n",
    "                pass\n",
    "        embedded_event = np.array(case_vector).mean(axis=0)\n",
    "        vectors.append(embedded_event)\n",
    "\n",
    "    embedded_sequence = np.array(vectors)\n",
    "    embedded_sequence = np.reshape(embedded_sequence, (vector_size*len(vectors)))\n",
    "    # print(\"Encoding:\", embedded_sequence.shape)\n",
    "    return embedded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(nr_features, hidden_layers, hidden_size_factor=0.5, noise=None):\n",
    "    '''\n",
    "    Create the DAE model\n",
    "    '''\n",
    "    input_ = Input(shape=(nr_features,), name='input')\n",
    "    x = input_\n",
    "\n",
    "    if noise is not None:\n",
    "        x = GaussianNoise(noise)(x) \n",
    "\n",
    "    for i in range(hidden_layers):\n",
    "        if isinstance(hidden_size_factor, list):\n",
    "            factor = hidden_size_factor[i]\n",
    "        else:\n",
    "            factor = hidden_size_factor\n",
    "        x = Dense(int(nr_features * factor), activation='relu', name=f'hid{i + 1}')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    output = Dense(nr_features, activation='tanh', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001, beta_2=0.99),\n",
    "        loss='mean_squared_error',\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-05T09:15:27.068387Z",
     "iopub.status.busy": "2023-07-05T09:15:27.067757Z",
     "iopub.status.idle": "2023-07-05T09:15:27.082010Z",
     "shell.execute_reply": "2023-07-05T09:15:27.081113Z",
     "shell.execute_reply.started": "2023-07-05T09:15:27.068357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to compute the reconstruction error for a sequence using the autoencoder model\n",
    "def compute_reconstruction_error(autoencoder, sequence):\n",
    "    reconstructed_sequence = autoencoder(sequence, training=False)\n",
    "    reconstruction_error = np.mean(np.square(sequence - reconstructed_sequence))\n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_oldest_event(current_id):\n",
    "    global case_ids, cid_to_events, event_counter\n",
    "\n",
    "    cid_with_oldest_event = case_ids.popleft()\n",
    "    while cid_with_oldest_event not in cid_to_events:\n",
    "        cid_with_oldest_event = case_ids.popleft()\n",
    "    cid_to_events[cid_with_oldest_event] = np.delete(cid_to_events[cid_with_oldest_event], 0, axis=0)\n",
    "    if len(cid_to_events[cid_with_oldest_event]) == 0 and cid_with_oldest_event != current_id:\n",
    "        del cid_to_events[cid_with_oldest_event]\n",
    "        del cid_to_last_timestamp[cid_with_oldest_event]\n",
    "    event_counter -= 1\n",
    "\n",
    "def save_new_event(cid, event_data, event_timestamp, after_GP: bool):\n",
    "    global cid_to_events, cid_to_last_timestamp, case_ids, event_counter\n",
    "    \n",
    "    event_counter += 1\n",
    "    case_ids.append(cid)\n",
    "    \n",
    "    if cid in cid_to_events.keys():\n",
    "        if event_counter >= w_events and after_GP:\n",
    "            remove_oldest_event(cid)\n",
    "        sequence = cid_to_events[cid]\n",
    "        sequence = np.vstack([sequence, event_data])\n",
    "        cid_to_events[cid] = sequence\n",
    "    else:\n",
    "        number_of_cases = len(cid_to_events.keys())\n",
    "        if number_of_cases >= w_cases and after_GP:\n",
    "            min_timestamp = datetime.strptime('2100-01-01 00:00:00', '%Y-%m-%d %H:%M:%S') # for datasets with this timestamp format\n",
    "            #min_timestamp = datetime.strptime('2100-01-01T00:00:00.111+09:00', '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "            removing_cid = None\n",
    "            for cid, latest_timestamp in cid_to_last_timestamp.items():\n",
    "                timestamp = datetime.strptime(latest_timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "                #timestamp = datetime.strptime(latest_timestamp, '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "                if timestamp < min_timestamp:\n",
    "                    min_timestamp = timestamp\n",
    "                    removing_cid = cid\n",
    "            del cid_to_events[removing_cid]\n",
    "            del cid_to_last_timestamp[removing_cid]\n",
    "        if event_counter >= w_events and after_GP:\n",
    "            remove_oldest_event(cid)\n",
    "        cid_to_events[cid] = np.array([event_data])\n",
    "        sequence = cid_to_events[cid]\n",
    "        cid_to_last_timestamp[cid] = event_timestamp\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_model_inputs(inputs, input_length_model):\n",
    "    # print(\"Padding: \", input_length_model, inputs.shape) \n",
    "    padded_vector= np.pad(inputs, (0, input_length_model - inputs.shape[0]), 'constant', constant_values=0)\n",
    "    encoded_sequence = np.expand_dims(padded_vector, axis=0)\n",
    "    return encoded_sequence\n",
    "\n",
    "def init_train_model(cid_to_events, input_dim, encoder):\n",
    "    model = make_model(input_dim, hidden_layers=2)\n",
    "    \n",
    "    max_prefix_length = 0\n",
    "    for v in cid_to_events.values():\n",
    "        if len(v) > max_prefix_length:\n",
    "            max_prefix_length = len(v) \n",
    "    \n",
    "    \n",
    "    for nr_events in range(1, max_prefix_length+1):\n",
    "        prefix_dict = {}\n",
    "        for key, value in cid_to_events.items():\n",
    "            if len(value) >= nr_events:\n",
    "                prefix_dict[key] = value[:nr_events]\n",
    "\n",
    "        for sequence in prefix_dict.values():\n",
    "            train_model(encoder, sequence)\n",
    "            embedding = concat_feature_vector(sequence, encoder)\n",
    "            tensor = pre_process_model_inputs(embedding, input_length_model = model.input.shape[1])\n",
    "            model.train_on_batch(tensor, tensor)\n",
    "            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-05T09:15:27.087334Z",
     "iopub.status.busy": "2023-07-05T09:15:27.086547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize DataFrame to store reconstruction errors\n",
    "# reconstruction_error_df = pd.DataFrame(columns=['case_id', 'reconstruction_error'])\n",
    "datasets = [dir_datasets + '/' + dataset for dataset in os.listdir(dir_datasets)]\n",
    "for dataset in datasets:\n",
    "    \n",
    "    file_name = os.path.basename(dataset)\n",
    "    df_dataset = pd.read_csv(dataset)\n",
    "    df_dataset.sort_values('timestamp', inplace=True)\n",
    "    #df_dataset = df_dataset[~df_dataset['case_id'].isin(df_dataset.groupby('case_id')['timestamp'].apply(lambda x: x.isna().any()).loc[lambda x: x].index)]\n",
    "    df_dataset['timestamp'] = pd.to_datetime(df_dataset['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # df_dataset = df_dataset.drop(['isEndTrace'], axis=1) #RCVDB\n",
    "    max_length = df_dataset.groupby('case_id').size().max()\n",
    "    cases = df_dataset.groupby(df_dataset.columns[0]).apply(lambda x: x.iloc[:, ~df_dataset.columns.isin(['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly'])].values.tolist())\n",
    "    word2vec = create_model(cases, vector_size, window, min_count)\n",
    "    input_dim = vector_size * max_length #RCVDB\n",
    "    results_filename = f'{dir_results}/results_w2v_{file_name}'\n",
    "\n",
    "    cid_to_events : dict[int, list]= dict()\n",
    "    cid_to_last_timestamp : dict[int, float] = dict() # to know what is for a case id the timestamp of the last event (the most recent)\n",
    "    case_ids = deque() # deque of the case ids such that, at each point, the popleft represents the oldest event\n",
    "    event_counter = 0\n",
    "    anomalous_cases = []\n",
    "\n",
    "    # Initialize DataFrame to store reconstruction errors\n",
    "    sliding_windows_dae = {}\n",
    "\n",
    "    for i in range(max_length + 1):\n",
    "        sliding_windows_dae[i] = metrics.Rolling(metrics.MSE(), window_size=6000)\n",
    "\n",
    "    after_GP = False\n",
    "    threshold = 1 # initial default value\n",
    "\n",
    "    # for _, row in df_dataset.iterrows():\n",
    "    for _, row in tqdm(df_dataset.iterrows(), total=len(df_dataset), desc=f\"Processing {dataset}\"):\n",
    "        \n",
    "        #skip_training = False\n",
    "        case_label = row['isAnomaly']\n",
    "        case_id = row['case_id']\n",
    "        timestamp = row['timestamp']\n",
    "        prefix_length = row['event_position']\n",
    "        anomaly_type = row['anomaly']\n",
    "        y_true = row['isAnomaly']\n",
    "        event_data = row.drop(['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly']).values\n",
    "        #event_data = row.iloc[list(range(2, 3)) + list(range(3, len(x.columns)))].values\n",
    "\n",
    "        sequence = save_new_event(case_id, event_data, timestamp, after_GP)\n",
    "        \n",
    "        if not after_GP:\n",
    "\n",
    "            if len([k for k, v in cid_to_events.items() if len(v) >= GP_length]) >= GP:\n",
    "                after_GP = True\n",
    "                autoencoder = init_train_model(cid_to_events, input_dim, encoder=word2vec)\n",
    "        \n",
    "        if after_GP:\n",
    "            start_time_encoding = time.time()    \n",
    "            # Encode the sequence using Word2Vec\n",
    "            encoded_sequence = concat_feature_vector(sequence, word2vec)\n",
    "            encoded_sequence = pre_process_model_inputs(encoded_sequence, input_length_model=autoencoder.input.shape[1])\n",
    "            train_model(word2vec, sequence)\n",
    "            start_time_scoring = time.time()\n",
    "            \n",
    "            # RCVDB:\n",
    "            # anomaly_score = compute_reconstruction_error(autoencoder, encoded_sequence)\n",
    "            reconstructed_sequence = autoencoder(encoded_sequence, training=False)\n",
    "            anomaly_score = np.mean(np.square(encoded_sequence - reconstructed_sequence))\n",
    "\n",
    "            as_window = sliding_windows_dae[prefix_length]\n",
    "\n",
    "            flat_encoded_sequence = encoded_sequence.flatten()\n",
    "            flat_reconstructed_sequence = reconstructed_sequence.numpy().flatten()\n",
    "            # print(flat_encoded_sequence.shape, flat_reconstructed_sequence.shape)\n",
    "\n",
    "            as_window.update(y_true=flat_encoded_sequence, y_pred=flat_reconstructed_sequence, sample_weight=1.0)\n",
    "\n",
    "            mean = np.mean(as_window.get())\n",
    "            std = np.std(as_window.get())\n",
    "            # print(f\"Mean: {mean}, Std: {std}\")\n",
    "            threshold = mean + alpha * std\n",
    "\n",
    "            # print(f\"Anomaly score: {anomaly_score}, Threshold: {threshold}\")\n",
    "            \n",
    "            if case_id in anomalous_cases:\n",
    "                predicted_label = 1\n",
    "                    \n",
    "            else:\n",
    "                predicted_label = 1 if anomaly_score >= threshold else 0\n",
    "            \n",
    "            #Saving the anomalous cases\n",
    "            if predicted_label == 1: \n",
    "                anomalous_cases.append(case_id)\n",
    "            \n",
    "            start_time_label = time.time()\n",
    "            \n",
    "            autoencoder.train_on_batch(encoded_sequence, encoded_sequence)\n",
    "            end_time_label = time.time()\n",
    "            encoding_duration = start_time_scoring - start_time_encoding\n",
    "            scoring_duration = start_time_label - start_time_scoring\n",
    "            prediction_duration = end_time_label - start_time_label\n",
    "\n",
    "            with open(results_filename, \"a+\") as csvfile:\n",
    "                csvfile.write(f\"{case_id},{prefix_length},{anomaly_score},{threshold},{predicted_label},{y_true},{anomaly_type},{encoding_duration},{scoring_duration},{prediction_duration}\\n\") #{mean_window},{std_window},"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyrako-online-ad-extension-3.9.21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
