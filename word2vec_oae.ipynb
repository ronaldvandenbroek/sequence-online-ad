{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:21.585881Z","iopub.status.busy":"2023-07-05T09:15:21.585516Z","iopub.status.idle":"2023-07-05T09:15:21.595022Z","shell.execute_reply":"2023-07-05T09:15:21.594098Z","shell.execute_reply.started":"2023-07-05T09:15:21.585849Z"},"trusted":true},"outputs":[],"source":["import os\n","import time\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from datetime import datetime\n","from gensim.models import Word2Vec\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, PReLU, Input, GaussianNoise\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from sklearn.utils import shuffle\n","from collections import deque\n","from river import utils\n","from river import stats"]},{"cell_type":"markdown","metadata":{},"source":["# Read Data "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#innit variables for algortihm\n","GP_length = 2 # length of the traces\n","GP = 100 #how many traces with length GP_length should be present before the algortihm starts\n","w_cases = 100 #maximum number of traces present in SW\n","w_events = 1000 #maximum number of events present in SW\n","alpha = 0.1\n","dir_datasets = '_Data/Later/Real'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:21.847326Z","iopub.status.busy":"2023-07-05T09:15:21.846985Z","iopub.status.idle":"2023-07-05T09:15:21.853846Z","shell.execute_reply":"2023-07-05T09:15:21.852829Z","shell.execute_reply.started":"2023-07-05T09:15:21.847295Z"},"trusted":true},"outputs":[],"source":["# Initialize the sequence dictionary and the autoencoder model\n","# Parameters for Word2Vec encoding\n","vector_size = 100\n","window = 5\n","min_count = 1\n","\n","# Parameters for the autoencoder\n"," #TODO find the actual maximum length\n","#encoding_dim = int(vector_size *0.2)\n","#hidden_dim_1 = int(encoding_dim *0.2) #\n","#hidden_dim_2 = int(hidden_dim_1 / 2)\n","#learning_rate = 1e-4"]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:21.855585Z","iopub.status.busy":"2023-07-05T09:15:21.855259Z","iopub.status.idle":"2023-07-05T09:15:27.024069Z","shell.execute_reply":"2023-07-05T09:15:27.023123Z","shell.execute_reply.started":"2023-07-05T09:15:21.855554Z"},"trusted":true},"outputs":[],"source":["def create_model(cases, size, window, min_count):\n","    model = Word2Vec(\n","                vector_size=size,\n","                window=window,\n","                min_count=min_count)\n","    sentences = []\n","    for group in cases:\n","        group_sentences = []\n","        for row in group:\n","            # Extract sentences from row and add to group_sentences\n","            row_sentences = [str(item) for item in row]\n","            group_sentences.extend(row_sentences)\n","        # Add group_sentences to sentences\n","        sentences.append(group_sentences)\n","    \n","    model.build_vocab(sentences)\n","    model.train(sentences, total_examples=len(sentences), epochs=10)\n","    return model\n","\n","def train_model(model, sequence): #array of arrays (events)\n","    #model.build_vocab(sentences, update=True)\n","    \"\"\"\n","    for event in sequence:\n","        print('Eventi:', event, len(event))\n","        sequence_event = []\n","        #for row in event:\n","            # Extract sentences from row and add to group_sentences\n","        row_sequence = [str(item) for item in event]\n","        print('atributet:', row_sequence)\n","        sequence_event.extend(row_sequence)\n","        print('Sekuence', sequence_event)\n","        # Add group_sentences to sentences\n","        #sequence.append(sequence_event)\n","    \"\"\"\n","    sequence = [[attribute for event in sequence for attribute in event]]\n","    model.train(sequence, total_examples=1, epochs=1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:27.025785Z","iopub.status.busy":"2023-07-05T09:15:27.025456Z","iopub.status.idle":"2023-07-05T09:15:27.031874Z","shell.execute_reply":"2023-07-05T09:15:27.030981Z","shell.execute_reply.started":"2023-07-05T09:15:27.025754Z"},"trusted":true},"outputs":[],"source":["def concat_feature_vector(events, model):\n","    vectors = []\n","    for event in events:\n","        case_vector = []\n","        for token in event:\n","            try:\n","                case_vector.append(model.wv[token])\n","            except KeyError:\n","                pass\n","        embedded_event = np.array(case_vector).mean(axis=0)\n","        vectors.append(embedded_event)\n","\n","    embedded_sequence = np.array(vectors)\n","    embedded_sequence = np.reshape(embedded_sequence, (vector_size*len(vectors)))\n","    return embedded_sequence"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:27.033970Z","iopub.status.busy":"2023-07-05T09:15:27.033353Z","iopub.status.idle":"2023-07-05T09:15:27.053172Z","shell.execute_reply":"2023-07-05T09:15:27.052249Z","shell.execute_reply.started":"2023-07-05T09:15:27.033940Z"},"trusted":true},"outputs":[],"source":["def average_feature_vector(events, model):\n","    vectors = np.empty((0, vector_size))\n","    for event in events:\n","        event_vector = []\n","        for token in event:\n","            try:\n","                event_vector.append(model.wv[token])\n","                #print('event vector', event_vector)\n","            except KeyError:\n","                event_vector.append(np.zeros(vector_size))\n","        embedded_event = np.array(event_vector)\n","        print('Embedded event shape: ', embedded_event.shape)\n","        vectors = np.vstack([vectors, embedded_event])\n","        #print('Vector of events:', vectors)\n","    print(\"Vectors shape\", np.array(vectors).shape)\n","    embedded_sequence = np.array(vectors).mean(axis=0) #taking the average of all the event\n","    print('Embedded sequence shape:', embedded_sequence.shape)\n","    # embedded_sequence = np.reshape(embedded_sequence, (100*len(vectors)))\n","    return embedded_sequence"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:27.055312Z","iopub.status.busy":"2023-07-05T09:15:27.054585Z","iopub.status.idle":"2023-07-05T09:15:27.066306Z","shell.execute_reply":"2023-07-05T09:15:27.065416Z","shell.execute_reply.started":"2023-07-05T09:15:27.055281Z"},"trusted":true},"outputs":[],"source":["# Function to create and train the autoencoder model\n","def create_autoencoder():\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Input(shape=(input_dim, )),\n","        tf.keras.layers.Dense(encoding_dim, activation='relu'),\n","        tf.keras.layers.PReLU(),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(hidden_dim_1, activation='relu'),\n","        tf.keras.layers.PReLU(),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(input_dim, activation='tanh')\n","    ])\n","\n","    #print(autoencoder.summary())\n","\n","    \n","    # define our early stopping\n","    early_stop = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        min_delta=0.0001,\n","        patience=100,\n","        verbose=1, \n","        mode='min',\n","        restore_best_weights=True)\n","    \n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(metrics=['accuracy'],\n","                        loss='mean_squared_error',\n","                        optimizer=optimizer,\n","                        )\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def make_model(nr_features, hidden_layers, hidden_size_factor=0.5, noise=None):\n","    '''\n","    Create a DAE model\n","    '''\n","    # Input layer\n","    input_ = Input(shape=(nr_features,), name='input')\n","    x = input_\n","\n","    # Noise layer\n","    if noise is not None:\n","        x = GaussianNoise(noise)(x) \n","\n","    # Hidden layers\n","    for i in range(hidden_layers):\n","        if isinstance(hidden_size_factor, list):\n","            factor = hidden_size_factor[i]\n","        else:\n","            factor = hidden_size_factor\n","        x = Dense(int(nr_features * factor), activation='relu', name=f'hid{i + 1}')(x)\n","        x = Dropout(0.5)(x)\n","\n","    # Output layer\n","    output = Dense(nr_features, activation='tanh', name='output')(x)\n","\n","    # Build model\n","    model = Model(inputs=input_, outputs=output)\n","\n","    # Compile model\n","    model.compile(\n","        optimizer=Adam(learning_rate=0.0001, beta_2=0.99),\n","        loss='mean_squared_error',\n","    )\n","\n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:27.068387Z","iopub.status.busy":"2023-07-05T09:15:27.067757Z","iopub.status.idle":"2023-07-05T09:15:27.082010Z","shell.execute_reply":"2023-07-05T09:15:27.081113Z","shell.execute_reply.started":"2023-07-05T09:15:27.068357Z"},"trusted":true},"outputs":[],"source":["# Function to compute the reconstruction error for a sequence using the autoencoder model\n","def compute_reconstruction_error(autoencoder, sequence):\n","    reconstructed_sequence = autoencoder(sequence, training=False)\n","    reconstruction_error = np.mean(np.square(sequence - reconstructed_sequence))\n","    return reconstruction_error"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import pytz\n","def get_timestamp(ts):\n","    try:\n","        # Try parsing with the first date format '%Y-%m-%d %H:%M:%S'\n","        timestamp = datetime.strptime(ts, '%Y-%m-%d %H:%M:%S')\n","    except ValueError:\n","            try:\n","                # If the first format fails, try parsing with the second date format '%Y-%m-%dT%H:%M:%S.%f%z'\n","                timestamp = datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S.%f%z')\n","            except ValueError:\n","                raise ValueError(\"No valid format found\")\n","    return timestamp"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def remove_oldest_event(current_id):\n","    global case_ids, cid_to_events, event_counter\n","\n","    cid_with_oldest_event = case_ids.popleft()\n","    while cid_with_oldest_event not in cid_to_events:\n","        cid_with_oldest_event = case_ids.popleft()\n","    cid_to_events[cid_with_oldest_event] = np.delete(cid_to_events[cid_with_oldest_event], 0, axis=0) # removing the oldest event\n","    if len(cid_to_events[cid_with_oldest_event]) == 0 and cid_with_oldest_event != current_id:\n","        del cid_to_events[cid_with_oldest_event]\n","        del cid_to_last_timestamp[cid_with_oldest_event]\n","    event_counter -= 1\n","\n","def save_new_event(cid, event_data, event_timestamp, after_GP: bool):\n","    global cid_to_events, cid_to_last_timestamp, case_ids, event_counter\n","    \n","    event_counter += 1\n","    case_ids.append(cid)\n","    #sequence = None\n","    \n","    if cid in cid_to_events.keys():\n","        # we only care about the number of events\n","        if event_counter >= w_events and after_GP:\n","            # remove the oldest event\n","            remove_oldest_event(cid)\n","        # otherwise event_counter < We:\n","        sequence = cid_to_events[cid]\n","        sequence = np.vstack([sequence, event_data])\n","        cid_to_events[cid] = sequence\n","    else:\n","        # we need to care about the number of cases and also the number of events\n","        number_of_cases = len(cid_to_events.keys())\n","        if number_of_cases >= w_cases and after_GP:\n","            # remove the case with the oldest last event\n","            min_timestamp = datetime.strptime('2100-01-01 00:00:00', '%Y-%m-%d %H:%M:%S') # for datasets with this timestamp format\n","            #min_timestamp = datetime.strptime('2100-01-01T00:00:00.111+09:00', '%Y-%m-%dT%H:%M:%S.%f%z')\n","            removing_cid = None\n","            for cid, latest_timestamp in cid_to_last_timestamp.items():\n","                timestamp = datetime.strptime(latest_timestamp, '%Y-%m-%d %H:%M:%S')\n","                #timestamp = datetime.strptime(latest_timestamp, '%Y-%m-%dT%H:%M:%S.%f%z')\n","                if timestamp < min_timestamp:\n","                    min_timestamp = timestamp\n","                    removing_cid = cid\n","            del cid_to_events[removing_cid]\n","            del cid_to_last_timestamp[removing_cid]\n","        # otherwise number_of_cases < W, check the number of events\n","        if event_counter >= w_events and after_GP:\n","            #remove the oldest event\n","            remove_oldest_event(cid)\n","        # otherwise event_counter < We:\n","        cid_to_events[cid] = np.array([event_data])\n","        sequence = cid_to_events[cid]\n","        cid_to_last_timestamp[cid] = event_timestamp\n","    return sequence"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from tensorflow import convert_to_tensor\n","def pre_process_model_inputs(inputs, input_length_model):\n","    padded_vector= np.pad(inputs, (0, input_length_model - inputs.shape[0]), 'constant', constant_values=0)\n","    encoded_sequence = np.expand_dims(padded_vector, axis=0)\n","    return encoded_sequence\n","    #return convert_to_tensor(final, dtype='float32')\n","\n","\n","from keras.backend import clear_session\n","from tensorflow import convert_to_tensor\n","'''\n","def init_train_model(window, encoder): #TODO fix the parameters\n","    #init models\n","    input_dim= sum([len(i) for i in encoder.categories_])*max_length\n","    model = create_autoencoder(input_dim, hidden_layers=2)\n","    #fit model for each prefix seen so far\n","    for sequence in window.values():\n","        #embedding = return_features_events(sequence)    \n","        embedding = onehot_encode_data(sequence, encoder)\n","        \n","        tensor = pre_process_model_inputs(embedding, input_dim)\n","        \n","        model.train_on_batch(tensor, tensor)\n","    clear_session()\n","    return model\n","'''\n","def init_train_model(cid_to_events, input_dim, encoder):\n","    model = make_model(input_dim, hidden_layers=2)\n","    \n","    max_prefix_length = 0\n","    for v in cid_to_events.values():\n","        if len(v) > max_prefix_length:\n","            max_prefix_length = len(v) \n","    \n","    \n","    for nr_events in range(1, max_prefix_length+1):\n","        prefix_dict = {}\n","        for key, value in cid_to_events.items():\n","            if len(value) >= nr_events:\n","                prefix_dict[key] = value[:nr_events]\n","\n","        for sequence in prefix_dict.values():\n","            train_model(encoder, sequence)\n","            embedding = concat_feature_vector(sequence, encoder)\n","            tensor = pre_process_model_inputs(embedding, input_length_model = model.input.shape[1])\n","            model.train_on_batch(tensor, tensor)\n","            \n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T09:15:27.087334Z","iopub.status.busy":"2023-07-05T09:15:27.086547Z"},"trusted":true},"outputs":[{"ename":"PermissionError","evalue":"[Errno 13] Permission denied: 'results_w2v_bpic17-0.3-1.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m scoring_duration \u001b[39m=\u001b[39m start_time_label \u001b[39m-\u001b[39m start_time_scoring\n\u001b[0;32m     87\u001b[0m prediction_duration \u001b[39m=\u001b[39m end_time_label \u001b[39m-\u001b[39m start_time_label\n\u001b[1;32m---> 89\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(results_filename, \u001b[39m\"\u001b[39;49m\u001b[39ma+\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m csvfile:\n\u001b[0;32m     90\u001b[0m     csvfile\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcase_id\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mprefix_length\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00manomaly_score\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mthreshold\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mpredicted_label\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mencoding_duration\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mscoring_duration\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mprediction_duration\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39m#{mean_window},{std_window},\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'results_w2v_bpic17-0.3-1.csv'"]}],"source":["# Initialize DataFrame to store reconstruction errors\n","# reconstruction_error_df = pd.DataFrame(columns=['case_id', 'reconstruction_error'])\n","datasets = [dir_datasets + '/' + dataset for dataset in os.listdir(dir_datasets)]\n","for dataset in datasets:\n","    \n","    file_name = os.path.basename(dataset)\n","    df_dataset = pd.read_csv(dataset)\n","    df_dataset.sort_values('timestamp', inplace=True)\n","    #df_dataset = df_dataset[~df_dataset['case_id'].isin(df_dataset.groupby('case_id')['timestamp'].apply(lambda x: x.isna().any()).loc[lambda x: x].index)]\n","    df_dataset['timestamp'] = pd.to_datetime(df_dataset['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n","    df_dataset = df_dataset.drop(['isEndTrace'], axis=1)\n","    max_length = df_dataset.groupby('case_id').size().max()\n","    cases = df_dataset.groupby(df_dataset.columns[0]).apply(lambda x: x.iloc[:, ~df_dataset.columns.isin(['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly'])].values.tolist())\n","    word2vec = create_model(cases, vector_size, window, min_count)\n","    input_dim = vector_size\n","    results_filename = 'results_w2v_' + file_name\n","\n","    cid_to_events : dict[int, list]= dict()\n","    cid_to_last_timestamp : dict[int, float] = dict() # to know what is for a case id the timestamp of the last event (the most recent)\n","    case_ids = deque() # deque of the case ids such that, at each point, the popleft represents the oldest event\n","    event_counter = 0\n","    anomalous_cases = []\n","\n","    # Initialize DataFrame to store reconstruction errors\n","    sliding_windows_dae = {}\n","\n","    for i in range(max_length + 1):\n","        sliding_windows_dae[i] = utils.Rolling(stats.Var(), window_size=6000)\n","\n","    after_GP = False\n","    threshold = 1 # initial default value\n","\n","    for _, row in df_dataset.iterrows():\n","        \n","        #skip_training = False\n","        case_label = row['isAnomaly']\n","        case_id = row['case_id']\n","        timestamp = row['timestamp']\n","        prefix_length = row['event_position']\n","        anomaly_type = row['anomaly']\n","        event_data = row.drop(['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly']).values\n","        #event_data = row.iloc[list(range(2, 3)) + list(range(3, len(x.columns)))].values\n","\n","        sequence = save_new_event(case_id, event_data, timestamp, after_GP)\n","        \n","        if not after_GP:\n","\n","            if len([k for k, v in cid_to_events.items() if len(v) >= GP_length]) >= GP:\n","                after_GP = True\n","                autoencoder = init_train_model(cid_to_events, input_dim, encoder=word2vec)\n","        \n","        if after_GP:\n","            start_time_encoding = time.time()    \n","            # Encode the sequence using Word2Vec\n","            encoded_sequence = concat_feature_vector(sequence, word2vec)\n","            encoded_sequence = pre_process_model_inputs(encoded_sequence, input_length_model=autoencoder.input.shape[1])\n","            train_model(word2vec, sequence)\n","            start_time_scoring = time.time()\n","            \n","            anomaly_score = compute_reconstruction_error(autoencoder, encoded_sequence)\n","            # Calculate the duration and append to the prediction_times list\n","        \n","\n","            \n","            \n","            as_window = sliding_windows_dae[prefix_length]\n","            std = np.sqrt(as_window.update(anomaly_score).get())\n","            mean = as_window.mean.get()\n","            threshold = mean + alpha * std\n","            \n","            if case_id in anomalous_cases:\n","                predicted_label = 1\n","                    \n","            else:\n","                predicted_label = 1 if anomaly_score >= threshold else 0\n","            \n","            #Saving the anomalous cases\n","            if predicted_label == 1: \n","                anomalous_cases.append(case_id)\n","            \n","            start_time_label = time.time()\n","            \n","            autoencoder.train_on_batch(encoded_sequence, encoded_sequence)\n","            end_time_label = time.time()\n","            encoding_duration = start_time_scoring - start_time_encoding\n","            scoring_duration = start_time_label - start_time_scoring\n","            prediction_duration = end_time_label - start_time_label\n","\n","            with open(results_filename, \"a+\") as csvfile:\n","                csvfile.write(f\"{case_id},{prefix_length},{anomaly_score},{threshold},{predicted_label}, {encoding_duration},{scoring_duration},{prediction_duration}\\n\") #{mean_window},{std_window},"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# with open(\"encoding_times-1.txt\", \"w\") as f:\n","    # for el in encoding_times:\n","       # f.write(str(el) + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# with open(\"prediction_times_thres-1.txt\", \"w\") as f:\n","   #  for el in prediction_times:\n","     #   f.write(str(el) + \"\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
