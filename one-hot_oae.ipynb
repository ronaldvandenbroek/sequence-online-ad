{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, PReLU, Input, GaussianNoise\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from river import utils\n",
    "from river import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#innit variables for algortihm\n",
    "GP_length = 2 # length of the traces\n",
    "GP = 100 #how many traces with length GP_length should be present before the algortihm starts\n",
    "w_cases = 100 #maximum number of traces present in SW\n",
    "w_events = 1000 #maximum number of events present in SW\n",
    "alpha = 0.1\n",
    "dir_datasets = '_Data/Later'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, hidden_layers, hidden_size_factor=0.2, noise=None):\n",
    "    '''\n",
    "    Create a DAE model\n",
    "    '''\n",
    "    # Input layer\n",
    "    input_ = Input(shape=(input_dim,), name='input')\n",
    "    x = input_\n",
    "\n",
    "    # Noise layer\n",
    "    if noise is not None:\n",
    "        x = GaussianNoise(noise)(x) \n",
    "\n",
    "    # Hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "        if isinstance(hidden_size_factor, list):\n",
    "            factor = hidden_size_factor[i]\n",
    "        else:\n",
    "            factor = hidden_size_factor\n",
    "        x = Dense(int(input_dim * factor), activation='relu', name=f'hid{i + 1}')(x)\n",
    "        #x = PReLU(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(input_dim, activation='tanh', name='output')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001, beta_2=0.99),\n",
    "        loss='mean_squared_error',\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the reconstruction error for a sequence using the autoencoder model\n",
    "def compute_anomaly_score(autoencoder, sequence):\n",
    "    reconstructed_sequence = autoencoder(sequence, training=False)\n",
    "    reconstruction_error = np.mean(np.square(sequence - reconstructed_sequence))\n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_oldest_event(current_id):\n",
    "    global case_ids, cid_to_events, event_counter\n",
    "\n",
    "    cid_with_oldest_event = case_ids.popleft()\n",
    "    while cid_with_oldest_event not in cid_to_events:\n",
    "        cid_with_oldest_event = case_ids.popleft()\n",
    "    cid_to_events[cid_with_oldest_event] = np.delete(cid_to_events[cid_with_oldest_event], 0, axis=0) # removing the oldest event\n",
    "    if len(cid_to_events[cid_with_oldest_event]) == 0 and cid_with_oldest_event != current_id:\n",
    "        del cid_to_events[cid_with_oldest_event]\n",
    "        del cid_to_last_timestamp[cid_with_oldest_event]\n",
    "    event_counter -= 1\n",
    "\n",
    "def save_new_event(cid, event_data, event_timestamp, after_GP: bool):\n",
    "    global cid_to_events, cid_to_last_timestamp, case_ids, event_counter\n",
    "    \n",
    "    event_counter += 1\n",
    "    case_ids.append(cid)\n",
    "    #sequence = None\n",
    "    \n",
    "    if cid in cid_to_events.keys():\n",
    "        # we only care about the number of events\n",
    "        if event_counter >= w_events and after_GP:\n",
    "            # remove the oldest event\n",
    "            remove_oldest_event(cid)\n",
    "        # otherwise event_counter < We:\n",
    "        sequence = cid_to_events[cid]\n",
    "        sequence = np.vstack([sequence, event_data])\n",
    "        cid_to_events[cid] = sequence\n",
    "        cid_to_last_timestamp[cid] = event_timestamp\n",
    "    else:\n",
    "        # we need to care about the number of cases and also the number of events\n",
    "        number_of_cases = len(cid_to_events.keys())\n",
    "        if number_of_cases >= w_cases and after_GP:\n",
    "            # remove the case with the oldest last event\n",
    "            min_timestamp = datetime.strptime('2100-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "            removing_cid = None\n",
    "            for cid, latest_timestamp in cid_to_last_timestamp.items():\n",
    "                timestamp = datetime.strptime(latest_timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "                if timestamp < min_timestamp:\n",
    "                    min_timestamp = timestamp\n",
    "                    removing_cid = cid\n",
    "            del cid_to_events[removing_cid]\n",
    "            del cid_to_last_timestamp[removing_cid]\n",
    "        # otherwise number_of_cases < W, check the number of events\n",
    "        if event_counter >= w_events and after_GP:\n",
    "            #remove the oldest event\n",
    "            remove_oldest_event(cid)\n",
    "        # otherwise event_counter < We:\n",
    "        cid_to_events[cid] = np.array([event_data])\n",
    "        sequence = cid_to_events[cid]\n",
    "        cid_to_last_timestamp[cid] = event_timestamp\n",
    "    return sequence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#%% return features of event for encoding\n",
    "\n",
    "def return_features_events(events):\n",
    "    if isinstance(events, pd.Series):\n",
    "        columns = sorted([index for index in events.index if index not in ['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly']])\n",
    "        return events[columns].to_frame().T\n",
    "    else:\n",
    "        return events.loc[:, ~events.columns.isin(['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly'])]\n",
    "\n",
    "def init_encoder(dataset):\n",
    "    dataset = return_features_events(dataset)\n",
    "    unique_list = []\n",
    "    for col in dataset.columns:\n",
    "        unique_list.append(pd.DataFrame(dataset[col].unique(), columns=[col]))\n",
    "    df_values = pd.concat(unique_list, axis=1)\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    encoder.fit(df_values)\n",
    "    return encoder\n",
    "\n",
    "def onehot_encode_data(sequence, encoder):\n",
    "    sequence_df = pd.DataFrame(sequence, columns=['name', 'day', 'user'])\n",
    "    embedded_features = encoder.transform(sequence_df)\n",
    "    return embedded_features.flatten()\n",
    "\n",
    "from tensorflow import convert_to_tensor\n",
    "def pre_process_model_inputs(inputs, input_length_model):\n",
    "    final = [inputs[i * input_length_model:(i + 1) * input_length_model] for i in range((len(inputs) + input_length_model - 1) // input_length_model )]\n",
    "    if len(final[-1]) < input_length_model:\n",
    "        final[-1] = np.append(final[-1], [0 for i in range(input_length_model-len(final[-1]))])\n",
    "    # return final\n",
    "    return convert_to_tensor(final, dtype='float32')\n",
    "\n",
    "\n",
    "from keras.backend import clear_session\n",
    "'''\n",
    "def init_train_model(window, encoder): #TODO fix the parameters\n",
    "    #init models\n",
    "    input_dim= sum([len(i) for i in encoder.categories_])*max_length\n",
    "    model = create_autoencoder(input_dim, hidden_layers=2)\n",
    "    #fit model for each prefix seen so far\n",
    "    for sequence in window.values():\n",
    "        #embedding = return_features_events(sequence)    \n",
    "        embedding = onehot_encode_data(sequence, encoder)\n",
    "        \n",
    "        tensor = pre_process_model_inputs(embedding, input_dim)\n",
    "        \n",
    "        model.train_on_batch(tensor, tensor)\n",
    "    clear_session()\n",
    "    return model\n",
    "'''\n",
    "def init_train_model(cid_to_events, encoder):\n",
    "    input_dim= sum([len(i) for i in encoder.categories_])*max_length\n",
    "    model = create_autoencoder(input_dim, hidden_layers=2)\n",
    "    \n",
    "    max_prefix_length = 0\n",
    "    for v in cid_to_events.values():\n",
    "        if len(v) > max_prefix_length:\n",
    "            max_prefix_length = len(v) \n",
    "    \n",
    "    \n",
    "    for nr_events in range(1, max_prefix_length+1):\n",
    "        prefix_dict = {}\n",
    "        for key, value in cid_to_events.items():\n",
    "            if len(value) >= nr_events:\n",
    "                prefix_dict[key] = value[:nr_events]\n",
    "\n",
    "        for sequence in prefix_dict.values():\n",
    "            embedding = onehot_encode_data(sequence, encoder)\n",
    "            tensor = pre_process_model_inputs(embedding, input_dim)\n",
    "            model.train_on_batch(tensor, tensor)\n",
    "    clear_session()        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [dir_datasets + '/' + dataset for dataset in os.listdir(dir_datasets)]\n",
    "for dataset in datasets:\n",
    "    \n",
    "    file_name = os.path.basename(dataset)\n",
    "    df_dataset = pd.read_csv(dataset)\n",
    "    df_dataset.sort_values('timestamp', inplace=True)\n",
    "    #df_dataset = df_dataset[~df_dataset['case_id'].isin(df_dataset.groupby('case_id')['timestamp'].apply(lambda x: x.isna().any()).loc[lambda x: x].index)]\n",
    "    df_dataset['timestamp'] = pd.to_datetime(df_dataset['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df = df_dataset.drop(['isEndTrace'], axis=1)\n",
    "    encoder = init_encoder(df)\n",
    "    max_length = df.groupby('case_id').size().max()\n",
    "    results_filename = 'results_onehot_' + file_name\n",
    "\n",
    "    cid_to_events : dict[int, list]= dict()\n",
    "    cid_to_last_timestamp : dict[int, float] = dict() # to know what is for a case id the timestamp of the last event (the most recent)\n",
    "    case_ids = deque() # deque of the case ids such that, at each point, the popleft represents the oldest event\n",
    "    event_counter = 0\n",
    "    anomalous_cases = []\n",
    "\n",
    "    # Initialize DataFrame to store reconstruction errors\n",
    "    sliding_windows_dae = {}\n",
    "\n",
    "    for i in range(max_length + 1):\n",
    "        sliding_windows_dae[i] = utils.Rolling(stats.Var(), window_size=6000)\n",
    "\n",
    "\n",
    "    after_GP = False\n",
    "    threshold = 1 # initial default value\n",
    "    for _, row in df.iterrows():\n",
    "        #skip_training = False\n",
    "        case_id = row['case_id']\n",
    "        timestamp = row['timestamp']\n",
    "        prefix_length = row['event_position']\n",
    "        event_data = row.drop(['case_id', 'event_position', 'timestamp', 'isAnomaly', 'anomaly']).values\n",
    "        \n",
    "        sequence = save_new_event(case_id, event_data, timestamp, after_GP)\n",
    "        \n",
    "        if not after_GP:\n",
    "            if len([k for k, v in cid_to_events.items() if len(v) >= GP_length]) >= GP:\n",
    "                after_GP = True\n",
    "                autoencoder = init_train_model(cid_to_events, encoder=encoder)\n",
    "        \n",
    "        if after_GP:\n",
    "            start_time_encoding = time.time()    \n",
    "            # Encode the sequence using Word2Vec\n",
    "            encoded_sequence = onehot_encode_data(sequence, encoder)\n",
    "            #print(encoded_sequence)\n",
    "            encoded_sequence = pre_process_model_inputs(encoded_sequence, input_length_model=autoencoder.input.shape[1])\n",
    "            \n",
    "            start_time_scoring = time.time()\n",
    "            \n",
    "            anomaly_score = compute_anomaly_score(autoencoder, encoded_sequence)\n",
    "        \n",
    "            predicted_label = 1 if anomaly_score >= threshold else 0\n",
    "            predicted_case_label = 1 if case_id in anomalous_cases else predicted_label\n",
    "            #Saving the anomalous cases\n",
    "            if predicted_label == 1: \n",
    "                anomalous_cases.append(case_id)\n",
    "            #Setting the threshold\n",
    "            as_window = sliding_windows_dae[prefix_length]\n",
    "            std = np.sqrt(as_window.update(anomaly_score).get())\n",
    "            mean = as_window.mean.get()\n",
    "            threshold = mean + alpha * std\n",
    "        \n",
    "            \n",
    "            start_time_label = time.time()\n",
    "            autoencoder.train_on_batch(encoded_sequence, encoded_sequence)\n",
    "            end_time_label = time.time()\n",
    "\n",
    "            encoding_duration = start_time_scoring - start_time_encoding\n",
    "            scoring_duration = start_time_label - start_time_scoring\n",
    "            prediction_duration = end_time_label - start_time_label\n",
    "\n",
    "\n",
    "            with open(results_filename, \"a+\") as csvfile:\n",
    "                csvfile.write(f\"{case_id},{prefix_length},{anomaly_score},{threshold},{predicted_label}, {predicted_case_label}, {encoding_duration},{scoring_duration},{prediction_duration}\\n\") #{mean_window},{std_window},\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenvt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
